Spark中的两个重要抽象是**RDD和共享变量**。

Spark提供了两种类型的变量：广播变量（broadcast variables）和累加器（accumulators）。
累加器则支持在所有不同节点之间进行累加计算（比如计数或者求和）。

###广播变量 
广播变量（broadcast variables）主要解决往task传输副本的效率问题,提前把变量包装成Broadcast然后传给每一个executor,
executor下的task都可以直接使用,只读
```scala
scala> val broadcastVar = sc.broadcast(Array(1, 2, 3))
broadcastVar: org.apache.spark.broadcast.Broadcast[Array[Int]] = Broadcast(0)
scala> broadcastVar.value
res0: Array[Int] = Array(1, 2, 3)
```

###累加器
用来实现计数器（counter）和求和（sum）。 Spark原生地支持数值型（numeric）的累加器，可以**自定义累加器对新类型的支持**。
创建累加器时指定了名字，则可以在Spark UI界面看到， 数值型累加器:使用SparkContext.longAccumulator() ,
SparkContext.doubleAccumulator()创建。 task只能做累加操作，不能读取累加器的值，只有任务控制节点（Driver Program）
可以使用value方法来读取累加器的值。
```scala
scala> val accum = sc.longAccumulator("My Accumulator")
accum: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 0, name: Some(My Accumulator), value: 0)
scala> sc.parallelize(Array(1, 2, 3, 4)).foreach(x => accum.add(x))
scala> accum.value
res1: Long = 10
```